{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2b7e5b-9e92-4e30-913c-5517d295013f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\B'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3564\\3975857625.py:5: SyntaxWarning: invalid escape sequence '\\B'\n",
      "  src = \"C:\\\\Users\\\\HP\\BLP Task 2\\\\test_v1.xlsx\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: tes_translated_generated_v2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "src = \"C:\\\\Users\\\\HP\\BLP Task 2\\\\test_v1.xlsx\"\n",
    "out = \"tes_translated_generated_v2.xlsx\"\n",
    "\n",
    "df = pd.read_excel(src)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _strip_wrapping_quotes(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n",
    "        return s[1:-1].strip()\n",
    "    return s\n",
    "\n",
    "def _strip_wrapping_backslashes(s: str) -> str:\n",
    "    # Remove any leading/trailing runs of backslashes\n",
    "    s = re.sub(r'^\\s*\\\\+', '', s)\n",
    "    s = re.sub(r'\\\\+\\s*$', '', s)\n",
    "    return s\n",
    "\n",
    "def _simple_unescape(s: str) -> str:\n",
    "    # Be conservative: only unescape the common sequences we expect to see\n",
    "    s = s.replace(r'\\n', '\\n').replace(r'\\t', '\\t').replace(r'\\\"', '\"').replace(r\"\\'\", \"'\")\n",
    "    # Remove spurious backslashes before brackets/commas/quotes that often appear in dirty CSV/Excel exports\n",
    "    s = re.sub(r'\\\\(?=[\\[\\]\\(\\)\\{\\},])', '', s)\n",
    "    return s\n",
    "\n",
    "def clean_test_list_text(raw) -> str:\n",
    "    \"\"\"Normalize the raw test_list cell into something parseable or at least searchable.\"\"\"\n",
    "    if pd.isna(raw):\n",
    "        return \"\"\n",
    "    s = str(raw)\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    s = _simple_unescape(s).strip()\n",
    "    # Sometimes cells come double-quoted twice, strip again\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    return s.strip()\n",
    "\n",
    "def literal_eval_list(s: str):\n",
    "    \"\"\"Try to literal_eval a Python list from string. Return list[str] or None.\"\"\"\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        # If it's a string that itself looks like a list, try once more\n",
    "        if isinstance(obj, str) and (obj.strip().startswith('[') or obj.strip().startswith('(')):\n",
    "            obj = ast.literal_eval(obj)\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            return list(obj)\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def regex_extract_first_assert(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Fallback: extract the first 'assert ...' using regex.\n",
    "    We grab up to a reasonable delimiter: newline, comma closing a quoted item, or list/tuple close.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # Make sure quotes are balanced-ish so regex works reasonably\n",
    "    # Capture lazily until a likely boundary\n",
    "    pat = r'assert\\s.*?(?=(?<!\\\\)\\n|(?<!\\\\)\\r|(?<!\\\\)\\'\\s*,|(?<!\\\\)\\\"\\s*,|(?<!\\\\)\\],|(?<!\\\\)\\)|(?<!\\\\)\\]|$)'\n",
    "    m = re.search(pat, s, flags=re.S)\n",
    "    if m:\n",
    "        candidate = m.group(0).strip()\n",
    "        # Trim dangling delimiters/quotes/brackets\n",
    "        candidate = re.sub(r'[\\s,\\'\\\"\\]\\)]*$', '', candidate).strip()\n",
    "        return candidate\n",
    "    return \"\"\n",
    "\n",
    "def extract_first_assert(raw) -> str:\n",
    "    \"\"\"\n",
    "    Robust extractor:\n",
    "    1) Clean text\n",
    "    2) Try literal_eval as a list and take the first string\n",
    "    3) If that fails/empty, regex fallback to find first assert\n",
    "    4) Final cleanup of leading/trailing quotes/backslashes\n",
    "    \"\"\"\n",
    "    s = clean_test_list_text(raw)\n",
    "    # First attempt: parse list\n",
    "    lst = literal_eval_list(s)\n",
    "    first = \"\"\n",
    "    if lst:\n",
    "        # Find the first non-empty string item that contains 'assert'\n",
    "        for item in lst:\n",
    "            if isinstance(item, str) and 'assert' in item:\n",
    "                first = item.strip()\n",
    "                break\n",
    "        # If first item is empty string due to stray slashes, try to clean it and keep if becomes non-empty\n",
    "        if not first and isinstance(lst[0], str):\n",
    "            cleaned0 = _strip_wrapping_quotes(_strip_wrapping_backslashes(_simple_unescape(lst[0]))).strip()\n",
    "            if 'assert' in cleaned0:\n",
    "                first = cleaned0\n",
    "\n",
    "    # Fallback to regex over the raw cleaned string if still empty\n",
    "    if not first:\n",
    "        first = regex_extract_first_assert(s)\n",
    "\n",
    "    # Final touch-ups\n",
    "    first = _strip_wrapping_quotes(first)\n",
    "    first = _strip_wrapping_backslashes(first).strip()\n",
    "\n",
    "    # Some rows may still carry trailing ellipses due to truncation; accept as-is if it starts with assert\n",
    "    if first and not first.startswith(\"assert\"):\n",
    "        # Occasionally the word assert is missing at the beginning due to trimming; try to recover\n",
    "        # by finding 'assert' inside and slicing from there.\n",
    "        pos = first.find(\"assert\")\n",
    "        if pos != -1:\n",
    "            first = first[pos:].strip()\n",
    "\n",
    "    # Ensure we only keep a single line if multiple lines leaked in\n",
    "    first = first.splitlines()[0].strip() if first else first\n",
    "    return first\n",
    "\n",
    "def strip_and_get_original_example(text):\n",
    "    \"\"\"Return text without Example line and also capture the old Example line (if any).\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\", None\n",
    "    lines = str(text).splitlines()\n",
    "    kept, old_example = [], None\n",
    "    for line in lines:\n",
    "        check = line.strip().strip('\"').strip(\"'\").strip()\n",
    "        if re.match(r'(?i)^example\\s*:', check):\n",
    "            old_example = check  # store original Example\n",
    "            continue\n",
    "        kept.append(line)\n",
    "    cleaned = \"\\n\".join(kept).strip()\n",
    "    cleaned = cleaned.strip('\"').strip(\"'\")\n",
    "    return cleaned, old_example\n",
    "\n",
    "# ---------- Transformation ----------\n",
    "new_texts = []\n",
    "first_asserts = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    base, old_ex = strip_and_get_original_example(row.get('instruction_en', ''))\n",
    "    first = extract_first_assert(row.get('test_list', ''))\n",
    "    first_asserts.append(first)\n",
    "\n",
    "    if first:  # Use extracted assert\n",
    "        new_texts.append(f\"{base}\\nExample: {first}\")\n",
    "    else:      # Fallback to old example if exists\n",
    "        if old_ex:\n",
    "            new_texts.append(f\"{base}\\n{old_ex}\")\n",
    "        else:\n",
    "            new_texts.append(base)\n",
    "\n",
    "df['first_assert'] = first_asserts\n",
    "df['instruction_en_new'] = new_texts\n",
    "\n",
    "df.to_excel(out, index=False)\n",
    "print(\"File saved to:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbfaeedf-5488-4ec1-bf3f-e0b9b85c4867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\B'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_3564\\4029583896.py:6: SyntaxWarning: invalid escape sequence '\\B'\n",
      "  src = \"C:\\\\Users\\\\HP\\BLP Task 2\\\\test_v1.xlsx\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: test_translated_generated_v1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# --- Paths ---\n",
    "src = \"C:\\\\Users\\\\HP\\BLP Task 2\\\\test_v1.xlsx\"\n",
    "out = \"test_translated_generated_v1.xlsx\"\n",
    "\n",
    "df = pd.read_excel(src)\n",
    "\n",
    "# ---------- Helpers (kept aligned with v3 behavior) ----------\n",
    "def _strip_wrapping_quotes(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n",
    "        return s[1:-1].strip()\n",
    "    return s\n",
    "\n",
    "def _strip_wrapping_backslashes(s: str) -> str:\n",
    "    # Remove any leading/trailing runs of backslashes\n",
    "    s = re.sub(r'^\\s*\\\\+', '', s)\n",
    "    s = re.sub(r'\\\\+\\s*$', '', s)\n",
    "    return s\n",
    "\n",
    "def _simple_unescape(s: str) -> str:\n",
    "    # Be conservative: only unescape the common sequences we expect to see\n",
    "    s = s.replace(r'\\n', '\\n').replace(r'\\t', '\\t').replace(r'\\\"', '\"').replace(r\"\\'\", \"'\")\n",
    "    # Remove spurious backslashes before brackets/commas/quotes that often appear in dirty CSV/Excel exports\n",
    "    s = re.sub(r'\\\\(?=[\\[\\]\\(\\)\\{\\},])', '', s)\n",
    "    return s\n",
    "\n",
    "def clean_test_list_text(raw) -> str:\n",
    "    \"\"\"Normalize the raw test_list cell into something parseable or at least searchable (same spirit as v3).\"\"\"\n",
    "    if pd.isna(raw):\n",
    "        return \"\"\n",
    "    s = str(raw)\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    s = _simple_unescape(s).strip()\n",
    "    # Sometimes cells come double-quoted twice, strip again\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    return s.strip()\n",
    "\n",
    "def literal_eval_list(s: str):\n",
    "    \"\"\"Try to literal_eval a Python list from string. Return list[str] or None.\"\"\"\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        # If it's a string that itself looks like a list, try once more\n",
    "        if isinstance(obj, str) and (obj.strip().startswith('[') or obj.strip().startswith('(')):\n",
    "            obj = ast.literal_eval(obj)\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            return list(obj)\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def regex_extract_all_asserts(s: str):\n",
    "    \"\"\"\n",
    "    Fallback: extract ALL 'assert ...' using the v3-style boundary pattern.\n",
    "    Capture lazily until a likely boundary: newline, comma closing a quoted item, or list/tuple close.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    pat = r'assert\\s.*?(?=(?<!\\\\)\\n|(?<!\\\\)\\r|(?<!\\\\)\\'\\s*,|(?<!\\\\)\\\"\\s*,|(?<!\\\\)\\],|(?<!\\\\)\\)|(?<!\\\\)\\]|$)'\n",
    "    matches = re.findall(pat, s, flags=re.S)\n",
    "    out = []\n",
    "    for m in matches:\n",
    "        candidate = m.strip()\n",
    "        # Trim dangling delimiters/quotes/brackets\n",
    "        candidate = re.sub(r'[\\s,\\'\\\"\\]\\)]*$', '', candidate).strip()\n",
    "        # Final touch-ups exactly like v3 spirit\n",
    "        candidate = _strip_wrapping_quotes(candidate)\n",
    "        candidate = _strip_wrapping_backslashes(candidate).strip()\n",
    "        if candidate and not candidate.startswith(\"assert\"):\n",
    "            pos = candidate.find(\"assert\")\n",
    "            if pos != -1:\n",
    "                candidate = candidate[pos:].strip()\n",
    "        if candidate:\n",
    "            out.append(candidate.splitlines()[0].strip())\n",
    "    return out\n",
    "\n",
    "def extract_all_asserts(raw):\n",
    "    \"\"\"\n",
    "    Robust extractor (v3-style, but for ALL asserts):\n",
    "      1) Clean text\n",
    "      2) Try literal_eval as a list -> collect strings containing 'assert'\n",
    "      3) If that fails/empty, regex fallback to find all asserts\n",
    "      4) Final cleanup of leading/trailing quotes/backslashes (already done)\n",
    "    \"\"\"\n",
    "    s = clean_test_list_text(raw)\n",
    "\n",
    "    # Attempt 1: parse list\n",
    "    found = []\n",
    "    lst = literal_eval_list(s)\n",
    "    if lst:\n",
    "        for item in lst:\n",
    "            if isinstance(item, str) and 'assert' in item:\n",
    "                it = item.strip()\n",
    "                # v3-style cleanup\n",
    "                it = _strip_wrapping_quotes(it)\n",
    "                it = _strip_wrapping_backslashes(_simple_unescape(it)).strip()\n",
    "                if it and not it.startswith(\"assert\"):\n",
    "                    pos = it.find(\"assert\")\n",
    "                    if pos != -1:\n",
    "                        it = it[pos:].strip()\n",
    "                if it:\n",
    "                    it = it.splitlines()[0].strip()\n",
    "                    if it.startswith('assert'):\n",
    "                        found.append(it)\n",
    "\n",
    "        # If the first element produced an empty string due to stray slashes, try cleaning it once more\n",
    "        if not found and len(lst) > 0 and isinstance(lst[0], str):\n",
    "            cleaned0 = _strip_wrapping_quotes(_strip_wrapping_backslashes(_simple_unescape(lst[0]))).strip()\n",
    "            if 'assert' in cleaned0:\n",
    "                pos = cleaned0.find(\"assert\")\n",
    "                if pos != -1:\n",
    "                    cleaned0 = cleaned0[pos:].strip()\n",
    "                cleaned0 = cleaned0.splitlines()[0].strip()\n",
    "                if cleaned0.startswith('assert'):\n",
    "                    found.append(cleaned0)\n",
    "\n",
    "    # Attempt 2: regex over cleaned string\n",
    "    if not found:\n",
    "        found = regex_extract_all_asserts(s)\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for a in found:\n",
    "        if a not in seen:\n",
    "            seen.add(a)\n",
    "            uniq.append(a)\n",
    "    return uniq\n",
    "\n",
    "# ---------- Transformation: keep Example as-is, append up to 3 asserts ----------\n",
    "new_texts = []\n",
    "all_asserts_col = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    base = str(row.get('instruction_en', '')).strip()  # KEEP as-is (including the Example line)\n",
    "    asserts = extract_all_asserts(row.get('test_list', ''))\n",
    "    all_asserts_col.append(\"\\n\".join(asserts))\n",
    "\n",
    "    if asserts:\n",
    "        appended = base + \"\\n\" + \"\\n\".join(asserts[:3])\n",
    "    else:\n",
    "        appended = base  # if no asserts, keep original unchanged\n",
    "\n",
    "    new_texts.append(appended)\n",
    "\n",
    "df['all_asserts_extracted'] = all_asserts_col\n",
    "df['instruction_en_appended'] = new_texts\n",
    "\n",
    "df.to_excel(out, index=False)\n",
    "print(\"File saved to:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a58680-b3a1-4129-8523-ed5276fe757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: test_translated_generated_v4.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "\n",
    "# --- Paths ---\n",
    "src = \"C:\\\\Users\\\\HP\\\\BLP Task 2\\\\test_v1.xlsx\"\n",
    "out = \"test_translated_generated_v4.xlsx\"\n",
    "\n",
    "df = pd.read_excel(src)\n",
    "\n",
    "# ---------- Helpers (kept aligned with v3 behavior, but less destructive) ----------\n",
    "def _strip_wrapping_quotes(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n",
    "        return s[1:-1].strip()\n",
    "    return s\n",
    "\n",
    "def _strip_wrapping_backslashes(s: str) -> str:\n",
    "    # Remove any leading/trailing runs of backslashes (common in messy exports)\n",
    "    s = re.sub(r'^\\s*\\\\+', '', s)\n",
    "    s = re.sub(r'\\\\+\\s*$', '', s)\n",
    "    return s\n",
    "\n",
    "def _simple_unescape(s: str) -> str:\n",
    "    # Be conservative: only unescape common sequences we expect to see\n",
    "    s = s.replace(r'\\n', '\\n').replace(r'\\t', '\\t').replace(r'\\\"', '\"').replace(r\"\\'\", \"'\")\n",
    "    # Remove spurious backslashes before brackets/commas/quotes that often appear in dirty CSV/Excel exports\n",
    "    s = re.sub(r'\\\\(?=[\\[\\]\\(\\)\\{\\},])', '', s)\n",
    "    return s\n",
    "\n",
    "def clean_test_list_text(raw) -> str:\n",
    "    \"\"\"Normalize the raw test_list cell into something parseable or at least searchable (same spirit as v3).\"\"\"\n",
    "    if pd.isna(raw):\n",
    "        return \"\"\n",
    "    s = str(raw)\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    s = _simple_unescape(s).strip()\n",
    "    # Sometimes cells come double-quoted twice, strip again\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    return s.strip()\n",
    "\n",
    "def literal_eval_list(s: str):\n",
    "    \"\"\"Try to literal_eval a Python list/tuple from string. Return list[str] or None.\"\"\"\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        # If it's a string that itself looks like a list, try once more\n",
    "        if isinstance(obj, str) and (obj.strip().startswith('[') or obj.strip().startswith('(')):\n",
    "            obj = ast.literal_eval(obj)\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            return list(obj)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Try JSON if literal_eval failed but it looks JSON-ish\n",
    "    try:\n",
    "        if s.strip().startswith('['):\n",
    "            obj = json.loads(s)\n",
    "            if isinstance(obj, list):\n",
    "                return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _clean_assert_text(it: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardize one assert string:\n",
    "      - ensure it starts with 'assert'\n",
    "      - strip only an outer closing quote if present (e.g., \"...']\" from list wrapping)\n",
    "    \"\"\"\n",
    "    it = it.strip()\n",
    "    # If not starting with assert but contains it, cut from first occurrence\n",
    "    if not it.startswith(\"assert\"):\n",
    "        pos = it.find(\"assert\")\n",
    "        if pos != -1:\n",
    "            it = it[pos:].strip()\n",
    "\n",
    "    # Remove a single trailing outer quote that belongs to a surrounding list item,\n",
    "    # but DO NOT remove internal brackets/parentheses from the assert itself.\n",
    "    # Examples to clean:  \"assert ...']\"  \"assert ...',\"  'assert ...\")'\n",
    "    it = re.sub(r'(?<!\\\\)([\"\\'])\\s*(?=[\\]\\)\\}],?|$)$', '', it)\n",
    "\n",
    "    return it.strip()\n",
    "\n",
    "def regex_extract_all_asserts(s: str):\n",
    "    \"\"\"\n",
    "    Safer fallback: grab 'assert ...' until newline, semicolon, or EOS.\n",
    "    Then, only strip an outer closing quote if present.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    # Match 'assert' followed by anything up to a newline, semicolon, or end of string.\n",
    "    # This avoids stopping at internal ] or ) which are part of the expression.\n",
    "    matches = re.findall(r'assert[^\\n\\r;]*', s, flags=re.S)\n",
    "    out = []\n",
    "    for m in matches:\n",
    "        candidate = _clean_assert_text(m)\n",
    "        if candidate.startswith(\"assert\"):\n",
    "            out.append(candidate)\n",
    "    return out\n",
    "\n",
    "def extract_all_asserts(raw):\n",
    "    \"\"\"\n",
    "    Robust extractor (v3-style, but for ALL asserts), fixed:\n",
    "      1) Clean text\n",
    "      2) Try literal_eval / JSON as a list -> collect strings containing 'assert'\n",
    "      3) If that fails/empty, regex fallback to find all asserts (line-based)\n",
    "      4) Deduplicate while preserving order\n",
    "    \"\"\"\n",
    "    s = clean_test_list_text(raw)\n",
    "\n",
    "    found = []\n",
    "\n",
    "    # Attempt 1: parse list\n",
    "    lst = literal_eval_list(s)\n",
    "    if lst:\n",
    "        for item in lst:\n",
    "            if isinstance(item, str) and 'assert' in item:\n",
    "                it = _simple_unescape(item)\n",
    "                it = _strip_wrapping_quotes(it)\n",
    "                it = _strip_wrapping_backslashes(it)\n",
    "                it = _clean_assert_text(it)\n",
    "                if it.startswith('assert'):\n",
    "                    found.append(it)\n",
    "\n",
    "        # If still nothing and first element is a string, try one more aggressive clean\n",
    "        if not found and len(lst) > 0 and isinstance(lst[0], str):\n",
    "            cleaned0 = _clean_assert_text(_simple_unescape(lst[0]))\n",
    "            if cleaned0.startswith('assert'):\n",
    "                found.append(cleaned0)\n",
    "\n",
    "    # Attempt 2: regex over cleaned string\n",
    "    if not found:\n",
    "        found = regex_extract_all_asserts(s)\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for a in found:\n",
    "        if a not in seen:\n",
    "            seen.add(a)\n",
    "            uniq.append(a)\n",
    "    return uniq\n",
    "\n",
    "# ---------- Transformation: keep Example as-is, append up to 3 asserts ----------\n",
    "new_texts = []\n",
    "all_asserts_col = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    base = str(row.get('instruction_en', '')).strip()  # KEEP as-is (including the Example line)\n",
    "    asserts = extract_all_asserts(row.get('test_list', ''))\n",
    "    all_asserts_col.append(\"\\n\".join(asserts))\n",
    "\n",
    "    if asserts:\n",
    "        appended = base + \"\\n\" + \"\\n\".join(asserts[:3])\n",
    "    else:\n",
    "        appended = base  # if no asserts, keep original unchanged\n",
    "\n",
    "    new_texts.append(appended)\n",
    "\n",
    "df['all_asserts_extracted'] = all_asserts_col\n",
    "df['instruction_en_appended'] = new_texts\n",
    "\n",
    "df.to_excel(out, index=False)\n",
    "print(\"File saved to:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a21db73e-eb6e-4cfd-85ba-0b42dffd5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: test_translated_generated_v5.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "\n",
    "# --- Paths (edit src as needed) ---\n",
    "# Example (Windows):\n",
    "# src = r\"C:\\Users\\HP\\BLP Task 2\\test_v1.xlsx\"\n",
    "# Example (this session / Colab):\n",
    "# src = \"/mnt/data/test_v1.xlsx\"\n",
    "src = \"C:\\\\Users\\\\HP\\\\BLP Task 2\\\\test_v1.xlsx\"\n",
    "out = \"test_translated_generated_v6.xlsx\"\n",
    "\n",
    "df = pd.read_excel(src)\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def _strip_wrapping_quotes(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n",
    "        return s[1:-1].strip()\n",
    "    return s\n",
    "\n",
    "def _strip_wrapping_backslashes(s: str) -> str:\n",
    "    s = re.sub(r'^\\s*\\\\+', '', s)\n",
    "    s = re.sub(r'\\\\+\\s*$', '', s)\n",
    "    return s\n",
    "\n",
    "def _simple_unescape(s: str) -> str:\n",
    "    s = s.replace(r'\\n', '\\n').replace(r'\\t', '\\t').replace(r'\\\"', '\"').replace(r\"\\'\", \"'\")\n",
    "    # remove spurious escapes before common punctuation/brackets\n",
    "    s = re.sub(r'\\\\(?=[\\[\\]\\(\\)\\{\\},])', '', s)\n",
    "    return s\n",
    "\n",
    "def clean_test_list_text(raw) -> str:\n",
    "    if pd.isna(raw):\n",
    "        return \"\"\n",
    "    s = str(raw)\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    s = _simple_unescape(s).strip()\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    return s.strip()\n",
    "\n",
    "def literal_eval_list(s: str):\n",
    "    \"\"\"Try Python literal, then JSON; return list or None.\"\"\"\n",
    "    # Python literal (e.g., \"['assert ...', 'assert ...']\")\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        if isinstance(obj, str) and (obj.strip().startswith('[') or obj.strip().startswith('(')):\n",
    "            obj = ast.literal_eval(obj)\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            return list(obj)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # JSON (e.g., [\"assert ...\", \"assert ...\"])\n",
    "    try:\n",
    "        if s.strip().startswith('['):\n",
    "            obj = json.loads(s)\n",
    "            if isinstance(obj, list):\n",
    "                return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _clean_assert_text(it: str) -> str:\n",
    "    \"\"\"Normalize one assert string without deleting internal brackets/quotes.\"\"\"\n",
    "    it = _simple_unescape(_strip_wrapping_backslashes(_strip_wrapping_quotes(str(it)))).strip()\n",
    "\n",
    "    # If 'assert' is inside, keep from first 'assert'\n",
    "    if not it.startswith(\"assert\"):\n",
    "        pos = it.find(\"assert\")\n",
    "        if pos != -1:\n",
    "            it = it[pos:].strip()\n",
    "\n",
    "    # Remove exactly one outer trailing quote that belongs to surrounding list, not inner code\n",
    "    it = re.sub(r'(?<!\\\\)([\"\\'])\\s*(?=[\\]\\)\\}],?|$)$', '', it)\n",
    "\n",
    "    # Drop trailing comma that belongs to list, not the code\n",
    "    it = re.sub(r',$', '', it).strip()\n",
    "\n",
    "    # One-line normalize (common for test cells)\n",
    "    return it.splitlines()[0].strip()\n",
    "\n",
    "def regex_extract_all_asserts(s: str):\n",
    "    \"\"\"\n",
    "    Fallback extractor: capture each 'assert' statement up to newline/semicolon/EOF.\n",
    "    Avoids stopping at internal ] or ) which are legitimate inside the expression.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    # Allow preceding punctuation/quotes; we start our capture from 'assert'\n",
    "    candidates = re.findall(r'assert[^\\n\\r;]*', s, flags=re.S)\n",
    "    out = []\n",
    "    for m in candidates:\n",
    "        a = _clean_assert_text(m)\n",
    "        if a.startswith(\"assert\"):\n",
    "            out.append(a)\n",
    "    return out\n",
    "\n",
    "def extract_all_asserts(raw):\n",
    "    \"\"\"\n",
    "    Robust extractor for ALL asserts in a cell:\n",
    "      1) Clean text\n",
    "      2) Try parsing list/tuple/JSON and collect strings with 'assert'\n",
    "      3) If none found, regex fallback on the cleaned string\n",
    "      4) Deduplicate while preserving order\n",
    "    \"\"\"\n",
    "    s = clean_test_list_text(raw)\n",
    "\n",
    "    found = []\n",
    "    lst = literal_eval_list(s)\n",
    "    if lst:\n",
    "        for item in lst:\n",
    "            if isinstance(item, str) and 'assert' in item:\n",
    "                a = _clean_assert_text(item)\n",
    "                if a.startswith(\"assert\"):\n",
    "                    found.append(a)\n",
    "\n",
    "        # If still empty, try a second pass on the joined string (some lists contain messy quoting)\n",
    "        if not found:\n",
    "            joined = \" \".join(map(str, lst))\n",
    "            found = regex_extract_all_asserts(clean_test_list_text(joined))\n",
    "    else:\n",
    "        # Not a clean list — use regex fallback\n",
    "        found = regex_extract_all_asserts(s)\n",
    "\n",
    "    # Deduplicate, keep order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for a in found:\n",
    "        if a not in seen:\n",
    "            seen.add(a)\n",
    "            uniq.append(a)\n",
    "    return uniq\n",
    "\n",
    "# -------------- Transformation --------------\n",
    "new_texts = []\n",
    "all_asserts_col = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    base = str(row.get('instruction_en', '')).strip()\n",
    "    asserts = extract_all_asserts(row.get('test_list', ''))\n",
    "    all_asserts_col.append(\"\\n\".join(asserts))\n",
    "    appended = base + (\"\\n\" + \"\\n\".join(asserts[:3]) if asserts else \"\")\n",
    "    new_texts.append(appended)\n",
    "\n",
    "df['all_asserts_extracted'] = all_asserts_col\n",
    "df['instruction_en_appended'] = new_texts\n",
    "\n",
    "df.to_excel(out, index=False)\n",
    "print(\"File saved to:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d011e8-8998-4a57-a5d7-daba25fdb643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: test_translated_generated_v6.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "\n",
    "# --- Paths (edit src as needed) ---\n",
    "# Example (Windows):\n",
    "# src = r\"C:\\Users\\HP\\BLP Task 2\\test_v1.xlsx\"\n",
    "# Example (this session):\n",
    "# src = \"/mnt/data/test_v1.xlsx\"\n",
    "src = r\"C:\\Users\\HP\\BLP Task 2\\test_v1.xlsx\"\n",
    "out = \"test_translated_generated_v6.xlsx\"\n",
    "\n",
    "df = pd.read_excel(src)\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def _strip_wrapping_quotes(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n",
    "        return s[1:-1].strip()\n",
    "    return s\n",
    "\n",
    "def _strip_wrapping_backslashes(s: str) -> str:\n",
    "    s = re.sub(r'^\\s*\\\\+', '', s)\n",
    "    s = re.sub(r'\\\\+\\s*$', '', s)\n",
    "    return s\n",
    "\n",
    "def _simple_unescape(s: str) -> str:\n",
    "    s = s.replace(r'\\n', '\\n').replace(r'\\t', '\\t').replace(r'\\\"', '\"').replace(r\"\\'\", \"'\")\n",
    "    s = re.sub(r'\\\\(?=[\\[\\]\\(\\)\\{\\},])', '', s)  # spurious escapes before brackets/commas\n",
    "    return s\n",
    "\n",
    "def clean_test_list_text(raw) -> str:\n",
    "    if pd.isna(raw):\n",
    "        return \"\"\n",
    "    s = str(raw)\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    s = _simple_unescape(s).strip()\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    return s.strip()\n",
    "\n",
    "def literal_eval_list(s: str):\n",
    "    \"\"\"Try Python literal, then JSON; return list or None.\"\"\"\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        if isinstance(obj, str) and (obj.strip().startswith('[') or obj.strip().startswith('(')):\n",
    "            obj = ast.literal_eval(obj)\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            return list(obj)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if s.strip().startswith('['):\n",
    "            obj = json.loads(s)\n",
    "            if isinstance(obj, list):\n",
    "                return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# ---------- Repair utilities ----------\n",
    "_OPENERS = {'(': ')', '[': ']', '{': '}'}\n",
    "_CLOSERS = {')': '(', ']': '[', '}': '{'}\n",
    "\n",
    "def _scan_balance(s: str):\n",
    "    \"\"\"\n",
    "    Returns (stack, in_single, in_double) after scanning s, ignoring brackets inside quotes.\n",
    "    'stack' contains unmatched openers in order.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    in_single = False\n",
    "    in_double = False\n",
    "    esc = False\n",
    "    for ch in s:\n",
    "        if esc:\n",
    "            esc = False\n",
    "            continue\n",
    "        if ch == '\\\\':\n",
    "            esc = True\n",
    "            continue\n",
    "        if not in_double and ch == \"'\" and not in_single:\n",
    "            in_single = True\n",
    "            continue\n",
    "        elif in_single and ch == \"'\":\n",
    "            in_single = False\n",
    "            continue\n",
    "        if not in_single and ch == '\"' and not in_double:\n",
    "            in_double = True\n",
    "            continue\n",
    "        elif in_double and ch == '\"':\n",
    "            in_double = False\n",
    "            continue\n",
    "        if in_single or in_double:\n",
    "            continue\n",
    "        if ch in _OPENERS:\n",
    "            stack.append(ch)\n",
    "        elif ch in _CLOSERS:\n",
    "            if stack and stack[-1] == _CLOSERS[ch]:\n",
    "                stack.pop()\n",
    "            else:\n",
    "                # unmatched closer; mark by pushing a sentinel of the closer with '!' prefix\n",
    "                stack.append('!' + ch)\n",
    "    return stack, in_single, in_double\n",
    "\n",
    "def _strip_trailing_unmatched_closers(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove only trailing unmatched ) ] } outside quotes (fixes extra 3rd bracket cases).\n",
    "    \"\"\"\n",
    "    # Work from the end, skipping whitespace and commas\n",
    "    i = len(s) - 1\n",
    "    esc = False\n",
    "    in_single = False\n",
    "    in_double = False\n",
    "    # First quick pass to know where quotes end\n",
    "    # We'll do a backwards walk, but simple approach:\n",
    "    # repeatedly remove trailing unmatched closers while scan indicates imbalance.\n",
    "    while True:\n",
    "        stack, in_s, in_d = _scan_balance(s)\n",
    "        if any(tok.startswith('!') for tok in stack):\n",
    "            # there is at least one unmatched closer somewhere; if it's trailing, drop it\n",
    "            m = re.search(r'[\\)\\]\\}]+\\s*$', s)\n",
    "            if m:\n",
    "                # remove one trailing closer\n",
    "                tail = m.group(0)\n",
    "                # remove a single char closer from the tail\n",
    "                for idx in range(len(tail)-1, -1, -1):\n",
    "                    if tail[idx] in _CLOSERS:\n",
    "                        s = s[:m.start() + idx] + tail[idx+1:]\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return s\n",
    "\n",
    "def _append_needed_closers(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Append the minimal set of missing quote/bracket closers to complete the line.\n",
    "    \"\"\"\n",
    "    # Close open quotes\n",
    "    stack, in_single, in_double = _scan_balance(s)\n",
    "    if in_single:\n",
    "        s += \"'\"\n",
    "    if in_double:\n",
    "        s += '\"'\n",
    "\n",
    "    # Re-scan after closing quotes (so brackets can be seen correctly)\n",
    "    stack, in_single, in_double = _scan_balance(s)\n",
    "    # If any unmatched closers ('!') remain, try stripping trailing ones\n",
    "    if any(tok.startswith('!') for tok in stack):\n",
    "        s = _strip_trailing_unmatched_closers(s)\n",
    "        stack, in_single, in_double = _scan_balance(s)\n",
    "\n",
    "    # Now close remaining openers\n",
    "    closers_to_add = ''.join(_OPENERS[o] for o in reversed([x for x in stack if not x.startswith('!')]))\n",
    "    if closers_to_add:\n",
    "        s += closers_to_add\n",
    "    return s\n",
    "\n",
    "def _ast_parses(s: str) -> bool:\n",
    "    try:\n",
    "        ast.parse(s)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _finalize_assert_line(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure the extracted line:\n",
    "      - starts with 'assert'\n",
    "      - has balanced quotes/brackets\n",
    "      - no trailing garbage commas/quotes/extra final bracket\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "\n",
    "    # If not starting with assert but contains it, keep from first 'assert'\n",
    "    if not s.startswith(\"assert\"):\n",
    "        pos = s.find(\"assert\")\n",
    "        if pos != -1:\n",
    "            s = s[pos:].strip()\n",
    "\n",
    "    # Remove trailing list punctuation from exports (outer quote/comma/bracket)\n",
    "    s = re.sub(r'\\s*([\"\\'])\\s*$', '', s)      # trailing outer quote\n",
    "    s = re.sub(r'\\s*,\\s*$', '', s)            # trailing comma\n",
    "    # Remove a single trailing unmatched closer if present (we'll re-balance anyway)\n",
    "    s = re.sub(r'\\s*[\\)\\]\\}]\\s*$', lambda m: '' if not _ast_parses(s) else m.group(0), s)\n",
    "\n",
    "    # Trim to one visual line (asserts are usually single-line in the sheet)\n",
    "    s = s.splitlines()[0].strip()\n",
    "\n",
    "    # Try parse; if fails, repair by balancing quotes/brackets\n",
    "    if not _ast_parses(s):\n",
    "        s = _strip_trailing_unmatched_closers(s)\n",
    "        s = _append_needed_closers(s)\n",
    "        # If still not parsable, last resort: stop at first semicolon\n",
    "        if not _ast_parses(s):\n",
    "            s = re.split(r'[;\\r\\n]', s)[0].strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "# ---------- Extractors ----------\n",
    "def regex_extract_all_asserts(s: str):\n",
    "    \"\"\"\n",
    "    Fallback extractor: capture 'assert' statements up to newline/semicolon/EOF,\n",
    "    then repair each (handles extra trailing brackets and missing quotes).\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    candidates = re.findall(r'assert[^\\n\\r;]*', s, flags=re.S)\n",
    "    out = []\n",
    "    for m in candidates:\n",
    "        a = _finalize_assert_line(_simple_unescape(_strip_wrapping_backslashes(_strip_wrapping_quotes(m))))\n",
    "        if a.startswith(\"assert\"):\n",
    "            out.append(a)\n",
    "    return out\n",
    "\n",
    "def extract_all_asserts(raw):\n",
    "    \"\"\"\n",
    "    Robust extractor for ALL asserts in a cell:\n",
    "      1) Clean text\n",
    "      2) Try parsing as list/tuple/JSON and collect strings with 'assert'\n",
    "      3) If none found, regex fallback\n",
    "      4) Normalize/repair each assert\n",
    "      5) Deduplicate while preserving order\n",
    "    \"\"\"\n",
    "    s = clean_test_list_text(raw)\n",
    "    found = []\n",
    "\n",
    "    lst = literal_eval_list(s)\n",
    "    if lst:\n",
    "        for item in lst:\n",
    "            if isinstance(item, str) and 'assert' in item:\n",
    "                a = _finalize_assert_line(item)\n",
    "                if a.startswith(\"assert\"):\n",
    "                    found.append(a)\n",
    "        if not found:  # messy list elements: join and fallback\n",
    "            joined = \" \".join(map(str, lst))\n",
    "            found = regex_extract_all_asserts(clean_test_list_text(joined))\n",
    "    else:\n",
    "        found = regex_extract_all_asserts(s)\n",
    "\n",
    "    # Deduplicate, keep order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for a in found:\n",
    "        if a not in seen:\n",
    "            seen.add(a)\n",
    "            uniq.append(a)\n",
    "    return uniq\n",
    "\n",
    "# -------------- Transformation --------------\n",
    "new_texts = []\n",
    "all_asserts_col = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    base = str(row.get('instruction_en', '')).strip()\n",
    "    asserts = extract_all_asserts(row.get('test_list', ''))\n",
    "    all_asserts_col.append(\"\\n\".join(asserts))\n",
    "    appended = base + (\"\\n\" + \"\\n\".join(asserts[:3]) if asserts else \"\")\n",
    "    new_texts.append(appended)\n",
    "\n",
    "df['all_asserts_extracted'] = all_asserts_col\n",
    "df['instruction_en_appended'] = new_texts\n",
    "\n",
    "df.to_excel(out, index=False)\n",
    "print(\"File saved to:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2168130-a6bd-42ea-a5d2-3c2c30011b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: test_translated_generated_v7.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "\n",
    "# --- Paths (edit src as needed) ---\n",
    "# Example (Windows):\n",
    "# src = r\"C:\\Users\\HP\\BLP Task 2\\test_v1.xlsx\"\n",
    "# Example (this session):\n",
    "# src = \"/mnt/data/test_v1.xlsx\"\n",
    "#src = r\"C:\\Users\\HP\\BLP Task 2\\test_v1.xlsx\"\n",
    "src = \"C:\\\\Users\\\\HP\\\\BLP Task 2\\\\test_250_v2.csv\"\n",
    "out = \"test_translated_generated_v7.xlsx\"\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def _strip_wrapping_quotes(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n",
    "        return s[1:-1].strip()\n",
    "    return s\n",
    "\n",
    "def _strip_wrapping_backslashes(s: str) -> str:\n",
    "    # Remove runs of backslashes that wrap the WHOLE cell or item, not the ones inside code\n",
    "    s = re.sub(r'^\\s*\\\\+', '', s)\n",
    "    s = re.sub(r'\\\\+\\s*$', '', s)\n",
    "    return s\n",
    "\n",
    "def _unescape_light_for_items(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Light unescape for ASSERT *items only*:\n",
    "      - Don't turn '\\n' or '\\t' into actual newlines/tabs (keeps asserts single-line).\n",
    "      - Do unescape \\\" and \\'\n",
    "      - Do drop spurious escapes before brackets/commas (common export artifact)\n",
    "    \"\"\"\n",
    "    s = s.replace(r'\\\"', '\"').replace(r\"\\'\", \"'\")\n",
    "    s = re.sub(r'\\\\(?=[\\[\\]\\(\\)\\{\\},])', '', s)\n",
    "    return s\n",
    "\n",
    "def clean_test_list_text(raw) -> str:\n",
    "    \"\"\"\n",
    "    Normalize the raw test_list cell into something parseable.\n",
    "    IMPORTANT: DO NOT unescape \\n at the cell level; it breaks literal_eval and regex boundaries.\n",
    "    \"\"\"\n",
    "    if pd.isna(raw):\n",
    "        return \"\"\n",
    "    s = str(raw)\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    # No unescape here (especially not \\n or \\t)\n",
    "    s = _strip_wrapping_quotes(s)\n",
    "    s = _strip_wrapping_backslashes(s)\n",
    "    return s.strip()\n",
    "\n",
    "def literal_eval_list(s: str):\n",
    "    \"\"\"Try Python literal, then JSON; return list or None.\"\"\"\n",
    "    try:\n",
    "        obj = ast.literal_eval(s)\n",
    "        if isinstance(obj, str) and (obj.strip().startswith('[') or obj.strip().startswith('(')):\n",
    "            obj = ast.literal_eval(obj)\n",
    "        if isinstance(obj, (list, tuple)):\n",
    "            return list(obj)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if s.strip().startswith('['):\n",
    "            obj = json.loads(s)\n",
    "            if isinstance(obj, list):\n",
    "                return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# ---------- Repair utilities ----------\n",
    "_OPENERS = {'(': ')', '[': ']', '{': '}'}\n",
    "_CLOSERS = {')': '(', ']': '[', '}': '{'}\n",
    "\n",
    "def _scan_balance(s: str):\n",
    "    \"\"\"\n",
    "    Returns (stack, in_single, in_double) after scanning s, ignoring brackets inside quotes.\n",
    "    'stack' contains unmatched openers in order; '!' + closer denotes an unmatched closer encountered.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    in_single = False\n",
    "    in_double = False\n",
    "    esc = False\n",
    "    for ch in s:\n",
    "        if esc:\n",
    "            esc = False\n",
    "            continue\n",
    "        if ch == '\\\\':\n",
    "            esc = True\n",
    "            continue\n",
    "        if not in_double and ch == \"'\" and not in_single:\n",
    "            in_single = True\n",
    "            continue\n",
    "        elif in_single and ch == \"'\":\n",
    "            in_single = False\n",
    "            continue\n",
    "        if not in_single and ch == '\"' and not in_double:\n",
    "            in_double = True\n",
    "            continue\n",
    "        elif in_double and ch == '\"':\n",
    "            in_double = False\n",
    "            continue\n",
    "        if in_single or in_double:\n",
    "            continue\n",
    "        if ch in _OPENERS:\n",
    "            stack.append(ch)\n",
    "        elif ch in _CLOSERS:\n",
    "            if stack and stack[-1] == _CLOSERS[ch]:\n",
    "                stack.pop()\n",
    "            else:\n",
    "                stack.append('!' + ch)\n",
    "    return stack, in_single, in_double\n",
    "\n",
    "def _strip_trailing_unmatched_closers(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove only trailing unmatched ) ] } outside quotes (fixes extra closing bracket cases).\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        stack, _, _ = _scan_balance(s)\n",
    "        if any(tok.startswith('!') for tok in stack):\n",
    "            m = re.search(r'[\\)\\]\\}]+\\s*$', s)\n",
    "            if m:\n",
    "                # remove just one trailing closer; loop will repeat if more exist\n",
    "                tail = m.group(0)\n",
    "                for idx in range(len(tail) - 1, -1, -1):\n",
    "                    if tail[idx] in _CLOSERS:\n",
    "                        s = s[:m.start() + idx] + tail[idx + 1:]\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return s\n",
    "\n",
    "def _append_needed_closers(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Append the minimal set of missing quote/bracket closers to complete the line.\n",
    "    \"\"\"\n",
    "    stack, in_single, in_double = _scan_balance(s)\n",
    "    if in_single:\n",
    "        s += \"'\"\n",
    "    if in_double:\n",
    "        s += '\"'\n",
    "    stack, _, _ = _scan_balance(s)\n",
    "    if any(tok.startswith('!') for tok in stack):\n",
    "        s = _strip_trailing_unmatched_closers(s)\n",
    "        stack, _, _ = _scan_balance(s)\n",
    "    closers_to_add = ''.join(_OPENERS[o] for o in reversed([x for x in stack if not x.startswith('!')]))\n",
    "    if closers_to_add:\n",
    "        s += closers_to_add\n",
    "    return s\n",
    "\n",
    "def _ast_parses(s: str) -> bool:\n",
    "    try:\n",
    "        ast.parse(s)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _finalize_assert_line(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure the extracted line:\n",
    "      - starts with 'assert'\n",
    "      - retains '\\n' as literal backslash-n (not real newline)\n",
    "      - has balanced quotes/brackets\n",
    "      - no trailing garbage commas/quotes/extra final bracket\n",
    "    \"\"\"\n",
    "    s = str(s)\n",
    "    s = _strip_wrapping_quotes(_strip_wrapping_backslashes(s))\n",
    "    s = _unescape_light_for_items(s)  # DO NOT turn '\\n' into actual newline\n",
    "    s = s.strip()\n",
    "\n",
    "    if not s.startswith(\"assert\"):\n",
    "        pos = s.find(\"assert\")\n",
    "        if pos != -1:\n",
    "            s = s[pos:].strip()\n",
    "\n",
    "    # Remove trailing outer quote/comma from list wrapping\n",
    "    s = re.sub(r'\\s*([\"\\'])\\s*$', '', s)\n",
    "    s = re.sub(r'\\s*,\\s*$', '', s)\n",
    "\n",
    "    # If there is an unmatched trailing closer, drop it; we'll re-balance later if needed\n",
    "    s = re.sub(r'\\s*[\\)\\]\\}]\\s*$', lambda m: '' if not _ast_parses(s) else m.group(0), s)\n",
    "\n",
    "    # Keep it single-line (asserts are meant to be one-liners in the sheet)\n",
    "    # IMPORTANT: We *did not* convert '\\n' to a real newline, so this won't split inside code.\n",
    "    s = s.splitlines()[0].strip()\n",
    "\n",
    "    if not _ast_parses(s):\n",
    "        s = _strip_trailing_unmatched_closers(s)\n",
    "        s = _append_needed_closers(s)\n",
    "        if not _ast_parses(s):\n",
    "            s = re.split(r'[;\\r\\n]', s)[0].strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "# ---------- Extractors ----------\n",
    "def regex_extract_all_asserts(s: str):\n",
    "    \"\"\"\n",
    "    Fallback extractor: capture 'assert' statements up to newline/semicolon/EOF,\n",
    "    then repair each (handles extra trailing brackets and missing quotes).\n",
    "    NOTE: We purposely do NOT unescape '\\n' before regex so matches stay on one visual line.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    candidates = re.findall(r'assert[^\\n\\r;]*', s, flags=re.S)\n",
    "    out = []\n",
    "    for m in candidates:\n",
    "        a = _finalize_assert_line(m)\n",
    "        if a.startswith(\"assert\"):\n",
    "            out.append(a)\n",
    "    return out\n",
    "\n",
    "def extract_all_asserts(raw):\n",
    "    \"\"\"\n",
    "    Robust extractor for ALL asserts in a cell:\n",
    "      1) Clean text (no unescape of \\n at cell level)\n",
    "      2) Try parsing as list/tuple/JSON and collect strings with 'assert'\n",
    "      3) If none found, regex fallback\n",
    "      4) Normalize/repair each assert\n",
    "      5) Deduplicate while preserving order\n",
    "    \"\"\"\n",
    "    s = clean_test_list_text(raw)\n",
    "    found = []\n",
    "\n",
    "    lst = literal_eval_list(s)\n",
    "    if lst:\n",
    "        for item in lst:\n",
    "            if isinstance(item, str) and 'assert' in item:\n",
    "                a = _finalize_assert_line(item)\n",
    "                if a.startswith(\"assert\"):\n",
    "                    found.append(a)\n",
    "        if not found:\n",
    "            # messy list elements: join and fallback\n",
    "            joined = \" \".join(map(str, lst))\n",
    "            found = regex_extract_all_asserts(clean_test_list_text(joined))\n",
    "    else:\n",
    "        found = regex_extract_all_asserts(s)\n",
    "\n",
    "    # Deduplicate, keep order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for a in found:\n",
    "        if a not in seen:\n",
    "            seen.add(a)\n",
    "            uniq.append(a)\n",
    "    return uniq\n",
    "\n",
    "# -------------- Transformation --------------\n",
    "new_texts = []\n",
    "all_asserts_col = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    base = str(row.get('instruction_en', '')).strip()\n",
    "    asserts = extract_all_asserts(row.get('test_list', ''))\n",
    "    all_asserts_col.append(\"\\n\".join(asserts))\n",
    "    appended = base + (\"\\n\" + \"\\n\".join(asserts[:3]) if asserts else \"\")\n",
    "    new_texts.append(appended)\n",
    "\n",
    "df['all_asserts_extracted'] = all_asserts_col\n",
    "df['instruction_en_appended'] = new_texts\n",
    "\n",
    "df.to_excel(out, index=False)\n",
    "print(\"File saved to:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48e26c-8561-423f-bfdc-6b33301c5fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
